Content-Type: text/x-zim-wiki
Wiki-Format: zim 0.6
Creation-Date: 2023-12-29T16:34:54-03:00

====== Configuração Plataforma K3s ======
Criado sexta 29 dezembro 2023

====== Arquitetura: ======

Ambiente de homologação utilizando k3s, deve servir como ponto de partida para configuração de produção on-premise (Recomendo RKE) ou nuvem (google/azure/aws)

* 3 nós Control Plane (exclusivo gestão do cluster)
* 3 nós Data Plane (exclusivo execução das aplicações)
* 2 nós Load Balance para conexão ao cluster
* 1 VM banco de dados
* 1 VM Storage NFS montado sob iscsi

===== IPs =====

10.50.255.100   cluster.datafrost-cic.idc
10.50.255.7     lb01.datafrost-cic.idc
10.50.255.8     lb02.datafrost-cic.idc

10.50.255.1     control01.datafrost-cic.idc
10.50.255.2     control02.datafrost-cic.idc
10.50.255.3     control03.datafrost-cic.idc

10.50.255.4     agent01.datafrost-cic.idc
10.50.255.5     agent02.datafrost-cic.idc
10.50.255.6     agent03.datafrost-cic.idc

10.50.255.9     database.datafrost-cic.idc
10.50.255.250   storage.datafrost-cic.idc



===== Load Balance =====

DTF-LAP-LB01-Dev HA-Proxy 10.50.255.7
DTF-LAP-LB02-Dev HA-Proxy 10.50.255.8

**IP Virtual:** 10.50.255.100

Haproxy
{{{code: lang="js" linenumbers="True"
frontend k3s-frontend
    bind *:6443
    mode tcp
    option tcplog
    default_backend k3s-backend

backend k3s-backend
    mode tcp
    option tcp-check
    balance roundrobin
    default-server inter 10s downinter 5s
    server control01 10.50.255.1:6443 check
    server control02 10.50.255.2:6443 check
    server control03 10.50.255.3:6443 check    
}}}

KeedAlived
{{{code: lang="js" linenumbers="True"
vrrp_script chk_haproxy {
    script 'killall -0 haproxy' # faster than pidof
    interval 2
}

vrrp_instance haproxy-vip {
   interface enp6s18
    state BACKUP # MASTER on lb-1, BACKUP on lb-2
    priority 100 # 200 on lb-1, 100 on lb-2

    virtual_router_id 51

    virtual_ipaddress {
        10.50.255.100/24
    }

    track_script {
        chk_haproxy
    }
}

}}}




====== Instruções ======

* Instalar kubectl
* Instalar o k (configuração bashrc)
* kubecolor se interessar

===== Instalação Control Plane =====

curl -sfL https://get.k3s.io | sh -s - server --cluster-init --tls-san=10.50.255.100 --node-taint CriticalAddonsOnly=true:NoExecute --disable=servicelb

===== Adicionando outros control plane =====

**Obter o token no primeiro servidor em:**
	cat /var/lib/rancher/k3s/server/node-token

curl -sfL https://get.k3s.io | K3S_TOKEN=K1010b0e089b498250842831c24b03279c73a038ba3c2d4a789675ee48261cf9214::server:79eec29da1d678c88f2538911f219dac sh -s - server --server https://10.50.255.100:6443 --tls-san=10.50.255.100 --node-taint CriticalAddonsOnly=true:NoExecute
	
===== Adicionando agentes =====

**Obter o token no primeiro servidor em:**
	cat /var/lib/rancher/k3s/server/node-token

curl -sfL https://get.k3s.io | K3S_TOKEN=K1010b0e089b498250842831c24b03279c73a038ba3c2d4a789675ee48261cf9214::server:79eec29da1d678c88f2538911f219dac sh -s - agent --server [[https://]][[https://10.50.255.100:6443|10.50.255.100]][[:6443]]


====== Configuração kubectl ======

Copiar do servidor **__(USUARIO ADMIN)__**

**~/.kube/config**
{{{code: lang="js" linenumbers="True"
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJkakNDQVIyZ0F3SUJBZ0lCQURBS0JnZ3Foa2pPUFFRREFqQWpNU0V3SHdZRFZRUUREQmhyTTNNdGMyVnkKZG1WeUxXTmhRREUzTURNNE9EVXlNREF3SGhjTk1qTXhNakk1TWpFeU5qUXdXaGNOTXpNeE1qSTJNakV5TmpRdwpXakFqTVNFd0h3WURWUVFEREJock0zTXRjMlZ5ZG1WeUxXTmhRREUzTURNNE9EVXlNREF3V1RBVEJnY3Foa2pPClBRSUJCZ2dxaGtqT1BRTUJCd05DQUFTc016cDhmYjI1RUJNZllPdnlmNkhYSnJDS2YrVTBiOVBZMkFNRnhOWlkKV1ZvN0RwZWttWWJRMmt3aTc2ZDVWUTVxUS90aXN5aHpkWmdNNThEeFFNSDNvMEl3UURBT0JnTlZIUThCQWY4RQpCQU1DQXFRd0R3WURWUjBUQVFIL0JBVXdBd0VCL3pBZEJnTlZIUTRFRmdRVVpMNlRUZ0ZXOG1oSkdFdDUvNytPClcva1VtL2d3Q2dZSUtvWkl6ajBFQXdJRFJ3QXdSQUlnRW1EcCtUNWJETGUwYjRWZ3B1Zk9sdm0zeFNtNlFYS3YKdEEzRDNjTXVSYkFDSUVrNmtQN1QwdEg2dGZDY2tpTWNPNTU0WExuYnkxdTM4UWJ0YXlJMWljMmgKLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
    server: https://10.50.255.100:6443
  name: alhambra
contexts:
- context:
    cluster: alhambra
    user: alhambra
  name: alhambra
current-context: default
kind: Config
preferences: {}
users:
- name: alhambra
  user:
    client-certificate-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJrVENDQVRlZ0F3SUJBZ0lJV2dwWERVdW9mUEV3Q2dZSUtvWkl6ajBFQXdJd0l6RWhNQjhHQTFVRUF3d1kKYXpOekxXTnNhV1Z1ZEMxallVQXhOekF6T0RnMU1qQXdNQjRYRFRJek1USXlPVEl4TWpZME1Gb1hEVEkwTVRJeQpPREl4TWpZME1Gb3dNREVYTUJVR0ExVUVDaE1PYzNsemRHVnRPbTFoYzNSbGNuTXhGVEFUQmdOVkJBTVRESE41CmMzUmxiVHBoWkcxcGJqQlpNQk1HQnlxR1NNNDlBZ0VHQ0NxR1NNNDlBd0VIQTBJQUJNZXZ5WWFialFXL1cya1EKQ2VTWE1SY0JNYjY0bnlzZWlFRlQrdFUrUjNPSTlyUWpSeXF3amhIVVlqZGZrTERERjl0TnFpdllHc3ZVNDRyTApqcnZhVGY2alNEQkdNQTRHQTFVZER3RUIvd1FFQXdJRm9EQVRCZ05WSFNVRUREQUtCZ2dyQmdFRkJRY0RBakFmCkJnTlZIU01FR0RBV2dCVFVnZkMzLzlKK0p6L280cURuOHhCZ2pnNHVsakFLQmdncWhrak9QUVFEQWdOSUFEQkYKQWlBSld0SUMzS1JEYXFZZ1JWeG5wWmR6QmJLRi9IeWUwWjVZc1lTOE9iZlIrd0loQVByM1lQQlA5THBLMmJ1ZApRcGZ4NUJTYW5VT3JtMm5HSkVLRXNDeFVJZlVpCi0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0KLS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJkakNDQVIyZ0F3SUJBZ0lCQURBS0JnZ3Foa2pPUFFRREFqQWpNU0V3SHdZRFZRUUREQmhyTTNNdFkyeHAKWlc1MExXTmhRREUzTURNNE9EVXlNREF3SGhjTk1qTXhNakk1TWpFeU5qUXdXaGNOTXpNeE1qSTJNakV5TmpRdwpXakFqTVNFd0h3WURWUVFEREJock0zTXRZMnhwWlc1MExXTmhRREUzTURNNE9EVXlNREF3V1RBVEJnY3Foa2pPClBRSUJCZ2dxaGtqT1BRTUJCd05DQUFUNHNDNnlyMExoQXoxSmRCNXRsdnZDd0t4MmZ5NXZUU25RblZ0NUxQYm8KbjRKT3dkbE1NT3lUSEhXTG5zN2hDdTlkd3JlVmN1RUNGdFRJMmMvWGY4VU5vMEl3UURBT0JnTlZIUThCQWY4RQpCQU1DQXFRd0R3WURWUjBUQVFIL0JBVXdBd0VCL3pBZEJnTlZIUTRFRmdRVTFJSHd0Ly9TZmljLzZPS2c1L01RCllJNE9McFl3Q2dZSUtvWkl6ajBFQXdJRFJ3QXdSQUlnSTIwMEFGUjdudFprQzBoQWZBcVAwQldGM0RMb0RNU0YKWXI4QUVQTkM4MVFDSUE3ckF2OSttblpkamNrcllpaTh6UjlidmxQZE1tVU9TNFAzODZwMk9uSncKLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
    client-key-data: LS0tLS1CRUdJTiBFQyBQUklWQVRFIEtFWS0tLS0tCk1IY0NBUUVFSUoyWUVTcVE3WnFrelFhTUpYU0xSMmNSTytESTczd0RCZTlUNjhka2xlYmRvQW9HQ0NxR1NNNDkKQXdFSG9VUURRZ0FFeDYvSmhwdU5CYjliYVJBSjVKY3hGd0V4dnJpZkt4NklRVlA2MVQ1SGM0ajJ0Q05IS3JDTwpFZFJpTjErUXNNTVgyMDJxSzlnYXk5VGppc3VPdTlwTi9nPT0KLS0tLS1FTkQgRUMgUFJJVkFURSBLRVktLS0tLQo=

}}}

====== Ambiente dev ======

* __TODO__: Documentar a configuração do traefki com virtualhost localmente no ambiente dev, 
	* Configurar [[/etc/hosts]] 127.0.0.1
	* O Traefik em ambientes de apenas um host (dev) vai pegar o ip da maquina local do loadbalancer
	* então ao criar um virtualhost para redirecionar o trafego de traefik.datafrost.local para porta 9000, ele usa o IP da maquina localmesmo 
	* NAVEGADOR => traefik.datafrost.local => 127.0.0.1 => Serviço do Traefik => processa virtualhost => redireciona porta 9000 => (IP [[LOCAL):9000]] transparente
	* podemos testar o proxy desta forma
	* k get nodes -o wide para obter o **ip** em INTERLAN-IP

